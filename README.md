# ðŸ’§ UNALWater - Medallion Architecture in Databricks

## 1. Introduction

**Project Description:**  
This project implements a modern Big Data architecture in **Databricks** using the **Medallion Architecture** (Bronze, Silver, Gold). It simulates water bottle sales events across MedellÃ­n to identify neighborhoods with the highest demand, aiming to optimize the marketing strategy of the fictional company **UNALWater**.

**Objectives:**
- Design a scalable solution for ingesting and analyzing simulated real-time sales data.
- Implement a complete data pipeline from raw ingestion to analytical dashboards.
- Generate valuable business insights through visualizations and analytical models.
- Apply data engineering best practices in a realistic Big Data environment.

---

## 2. Project Structure

```bash
/city-water-databricks-medallion-lakehouse/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”‚   â””â”€â”€ bronze_streaming.py
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_customers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ silver_employees.py
â”‚   â”‚   â”‚   â””â”€â”€ silver_streaming.py
â”‚   â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”‚   â”œâ”€â”€ gold_customers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gold_employees.py
â”‚   â”‚   â”‚   â””â”€â”€ gold_performance_operation.py
â”‚   â”‚   â””â”€â”€ transversal/
â”‚   â”‚       â”œâ”€â”€ clean_directories_and_tables.py
â”‚   â”‚       â”œâ”€â”€ config.py
â”‚   â”‚       â”œâ”€â”€ data_simulator.py
â”‚   â”‚       â”œâ”€â”€ load_data.py
â”‚   â”‚       â””â”€â”€ utils.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ data_processing.py
â”‚       â”œâ”€â”€ model_training.py
â”‚       â””â”€â”€ demand_prediction.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 50001.parquet
â”‚   â”œâ”€â”€ bronze.snappy.parquet
â”‚   â”œâ”€â”€ silver.snappy.parquet
â”‚   â”œâ”€â”€ medellin_neighborhoods.parquet
â”‚   â”œâ”€â”€ customers.parquet
â”‚   â””â”€â”€ employees.parquet
â”œâ”€â”€ volumes/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ architecture_diagram.png
â””â”€â”€ README.md
```

**Component Description:**
- `notebooks/`: Databricks notebooks for each stage of the data pipeline.
- `utils/`: Reusable helper modules (e.g., polygon mapping).
- `data/`: Simulated data and geojson files for geospatial analysis.
- `volumes/`: Stores data in Bronze, Silver, and Gold layers.
- `diagrams/`: Visual architecture diagrams.
- `README.md`: Project documentation.

---

## 3. Pipelines

**Stages:**
- **Event Simulation**: Generate synthetic sales events every 30 seconds.
- **Bronze Ingestion**: Store raw data in Delta Lake using Spark Structured Streaming.
- **Silver Transformation**: Clean and enrich with geospatial context.
- **Gold Aggregation**: Compute KPIs and summary tables.
- **Visualization**: Explore results using Databricks SQL dashboards.

**Pipeline Flow Diagram:**

```
[ Simulated JSON Events ]
          â†“
[ Bronze Layer - Raw ]
          â†“
[ Silver Layer - Clean + Geo ]
          â†“
[ Gold Layer - Metrics ]
          â†“
[ Dashboards & Analysis ]
```

**Stage Details:**
- **Simulation**: Python script generates sale records with metadata and coordinates.
- **Bronze**: Real-time ingestion and Delta Lake storage.
- **Silver**: Parse dates and enrich with spatial joins (GeoJSON).
- **Gold**: Metrics by district/time and export to SQL tables.
- **Visualization**: Dashboards with KPIs, maps, and interactive widgets.

**Technologies Used:**
- Apache Spark (Structured Streaming)
- Delta Lake
- Python
- SQL (Databricks)
- GeoJSON + GeoSpark or `ST_Contains`
- Databricks SQL Widgets

---

## 4. Databases

**Schemas:**
- `bronze_db`: Raw ingested data
- `silver_db`: Clean and enriched data
- `gold_db`: Aggregated analytical data

**Key Queries:**
- Top neighborhoods by product sales.
- Aggregation by hour/day of week.
- Dashboard filters using SQL widgets.

---

## 5. Setup & Deployment

**Environment Setup:**
- Databricks cluster with Runtime 12+ and Spark 3.x
- Upload scripts and GeoJSON files
- Create volumes in Databricks UI
- Configure SQL widgets and permissions (if using dashboards)

**Deployment Steps:**
1. Run `01_data_simulation.py`
2. Execute pipeline in order:
   - `02_bronze_ingestion.py`
   - `03_silver_processing.py`
   - `04_gold_analytics.py`
3. Visualize with `05_dashboard_visualization.sql`

**Requirements:**
- `geopandas` or `pyspark.sql.functions` with geospatial extensions
- Access to Databricks Volumes
- GeoJSON data for MedellÃ­n districts

---

## 6. Maintenance & Updates

**Maintenance:**
- Ensure streaming jobs are active
- Monitor checkpoints and lag via Spark UI
- Check for null/inconsistent values in Silver layer

**Updates:**
- Enhance simulator for new data patterns
- Retrain models if consumption behavior changes
- Add new KPIs and visualizations

---

## 7. References

- [Databricks Structured Streaming](https://docs.databricks.com/)
- [Medallion Architecture Overview](https://www.databricks.com/glossary/medallion-architecture)
- [Delta Lake](https://delta.io/)
- [Geospatial joins in Spark](https://databricks.com/blog/2021/09/27/performing-geospatial-joins-in-apache-spark.html)
