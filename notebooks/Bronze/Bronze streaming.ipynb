{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1838fbe-d7cf-4efd-af85-be635810a93c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook explanation\n",
    "\n",
    "This notebook is designed to process and analyze orders using Databricks and Apache Spark.\n",
    "\n",
    "## 1. Data intake in streaming (Bronze Layer)\n",
    "\n",
    "The `bronze_stream` function is defined to read data in JSON format from an landing zone (Landing Zone) using Spark Structured Streaming. The data scheme includes geographic information, date, customer and employee identifiers, number of products and the order identifier.\n",
    "\n",
    "- ** Duplicate elimination: ** Duplicates are eliminated based on `Order_id`.\n",
    "- ** Writing in Delta Lake: ** Clean data is written in a delta table, allowing efficient consultations and handling large data volumes.\n",
    "- ** Checkpointing: ** A CheckPoint location is used for failure tolerance and stream secure reset.\n",
    "\n",
    "## 2. Stream Execution\n",
    "\n",
    "The `bronze_stream` function is executed by passing the corresponding routes and table names to initiate the intake and storage process of orders.\n",
    "\n",
    "---\n",
    "\n",
    "This flow allows you to analyze feelings of reviews and process orders in real time, facilitating data -based decision making and the integration of advanced analysis into business data pipelines.\n",
    "\"\"\n",
    "\n",
    "With Open (\"/workspace/explanation_notebook.md\", \"w\", encoding = \"utf-8\") as f:\n",
    "    F.Write (Notebook_Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d370e52-2b6c-4f89-86ea-460a80b99ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run ../Transversal/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f2ec2a-ffeb-4564-99a2-525ff3dfe280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "\n",
    "def bronze_stream(landing_zone, checkpoint, table):\n",
    "\n",
    "    schema = StructType() \\\n",
    "        .add(\"latitude\", DoubleType()) \\\n",
    "        .add(\"longitude\", DoubleType()) \\\n",
    "        .add(\"date\", StringType()) \\\n",
    "        .add(\"customer_id\", StringType()) \\\n",
    "        .add(\"employee_id\", StringType()) \\\n",
    "        .add(\"quantity_products\", IntegerType()) \\\n",
    "        .add(\"order_id\", StringType())\n",
    "\n",
    "    raw_stream_df = (\n",
    "        spark.readStream\n",
    "            .format(\"json\")\n",
    "            .schema(schema)\n",
    "            .option(\"path\", landing_zone)\n",
    "            .load()\n",
    "    )\n",
    "\n",
    "    clean_stream_df = raw_stream_df.dropDuplicates([\"order_id\"])  # It must be changed to Watermark and Window when they are many records.\n",
    "\n",
    "    query = (\n",
    "        clean_stream_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint)\n",
    "            .trigger(once=True)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .toTable(table))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "query = bronze_stream(\n",
    "            landing_zone=landing_zone_path,\n",
    "            checkpoint=checkpoint_bronze,\n",
    "            table=Bronze_Orders)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze streaming",
   "widgets": {}
  },
  "dependencies": {
   "environment": {
    "environmentId": "913dd7af-cd09-411a-93a1-ccfc2c7af3d7",
    "workspaceId": "19f7c29b-83a9-42bb-812f-ef7bfc71c961"
   },
   "lakehouse": {
    "default_lakehouse": "5d727189-ca80-47e8-b94d-a7289df703a2",
    "default_lakehouse_name": "unalwater",
    "default_lakehouse_workspace_id": "19f7c29b-83a9-42bb-812f-ef7bfc71c961",
    "known_lakehouses": [
     {
      "id": "5d727189-ca80-47e8-b94d-a7289df703a2"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
