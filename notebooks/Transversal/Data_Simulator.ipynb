{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5baf789-27a9-4d5c-b563-77874e62fc65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explanation of the operation of this notebook\n",
    "\n",
    "This notebook is designed to guide the user through a specific workflow, which can include data loading, exploratory analysis, visualization and construction of models. Throughout the cells, instructions and code examples are presented to facilitate the understanding and execution of each step. The objective is to provide an interactive and reproducible tool for data analysis or the resolution of a particular problem.\n",
    "\n",
    "Be sure to execute the cells in order and follow the indications provided to obtain the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c36dea-1914-4f2d-98d1-b6a32e53adbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b9f886-55ab-42f5-bca1-84ff5712bca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.21 in /databricks/python3/lib/python3.12/site-packages (from shapely) (1.26.4)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.databricks.v1+h3_hint": "",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3501bf66-2fae-4cc4-9c96-e3d5f3e4f1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "customers_df = spark.table(Bronze_Customers)\n",
    "valid_customer_ids = [row[\"customer_id\"] for row in customers_df.select(\"customer_id\").collect()]\n",
    "\n",
    "employees_df = spark.table(Bronze_Employes)\n",
    "valid_employee_ids = [row[\"employee_id\"] for row in employees_df.select(\"employee_id\").collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b7f30f-b8e0-45a4-91a9-780806eee490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "medellin_spark_df = spark.table(Bronze_Medellin)\n",
    "gdf_medellin = spark_to_geopandas(df_spark=medellin_spark_df)\n",
    "\n",
    "polygon = gdf_medellin.geometry.union_all()\n",
    "minx, miny, maxx, maxy = polygon.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddfd030-f0a9-4ddf-96df-b9177df44f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 216 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 215 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 214 bytes.\n",
      "Wrote 213 bytes.\n",
      "Wrote 212 bytes.\n",
      "Wrote 215 bytes.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import time\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def generar_eventos(max_events=events, intervalo=intervalo):\n",
    "  \n",
    "  for _ in range(max_events):\n",
    "\n",
    "    generated_order_id = str(uuid.uuid4())\n",
    "    generated_event_date = datetime.now(horario_colombia).strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    generated_quantity = random.randint(14, 107)\n",
    "    chosen_customer_id = random.choice(valid_customer_ids)\n",
    "    chosen_employee_id = random.choice(valid_employee_ids)\n",
    "\n",
    "    while True:\n",
    "      point = Point(random.uniform(minx, maxx), random.uniform(miny, maxy))\n",
    "      if polygon.contains(point):\n",
    "        generated_latitude = point.y\n",
    "        generated_longitude = point.x\n",
    "        break\n",
    "      \n",
    "    event_data = {\n",
    "    \"latitude\": generated_latitude,\n",
    "    \"longitude\": generated_longitude,\n",
    "    \"date\": generated_event_date,\n",
    "    \"customer_id\": chosen_customer_id,\n",
    "    \"employee_id\": chosen_employee_id,\n",
    "    \"quantity_products\": generated_quantity,\n",
    "    \"order_id\": generated_order_id\n",
    "    }\n",
    "      \n",
    "    file_path = f\"{landing_zone_path}/{generated_order_id}.json\" \n",
    "    dbutils.fs.put(file_path, json.dumps(event_data), overwrite=True)\n",
    "      \n",
    "    time.sleep(intervalo)\n",
    "\n",
    "\n",
    "generar_eventos()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Simulator",
   "widgets": {}
  },
  "dependencies": {
   "environment": {
    "environmentId": "913dd7af-cd09-411a-93a1-ccfc2c7af3d7",
    "workspaceId": "19f7c29b-83a9-42bb-812f-ef7bfc71c961"
   },
   "lakehouse": {
    "default_lakehouse": "5d727189-ca80-47e8-b94d-a7289df703a2",
    "default_lakehouse_name": "unalwater",
    "default_lakehouse_workspace_id": "19f7c29b-83a9-42bb-812f-ef7bfc71c961",
    "known_lakehouses": [
     {
      "id": "5d727189-ca80-47e8-b94d-a7289df703a2"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
