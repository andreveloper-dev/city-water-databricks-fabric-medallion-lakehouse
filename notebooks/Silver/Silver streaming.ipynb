{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4681c4ba-146a-4375-8f95-52132d136111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook explanation\n",
    "\n",
    "This notebook performs geospatial and orders data processing using PysPark and Delta Lake in Databricks.\n",
    "\n",
    "## 1. Geodatos processing\n",
    "- A geographical data table (`bronze_geodata`) is loaded and becomes a geodataframe.\n",
    "- Neighborhood and communes are extracted and transmitted as a Broadcast variable for efficiency.\n",
    "- A UDF (`` community_barrio`) is defined that, given a latitude and length, determines to which commune and neighborhood the geographical point belongs.\n",
    "\n",
    "## 2. Order processing (streaming)\n",
    "- The `Silver_stream` function is defined to read streaming orders from a delta table (` Table_Bronze`).\n",
    "- Duplicate are eliminated by `Order_id`.\n",
    "- The date column is converted to Timestamp and each order is enriched with the geographical information of commune and neighborhood using the UDF defined.\n",
    "- Relevant columns are selected and transformed, including decomposition of the date in year, month, day, time, minute and second.\n",
    "- Finally, enriched data is written in a delta table (`Table_Silver`) in Append mode, with version control and tolerance for scheme changes.\n",
    "\n",
    "This flow allows you to maintain a Silver table of enriched orders with geographic information, ready for subsequent analyzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a168cd-b61d-4936-a22f-9156640f1d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run ../Transversal/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273d3cb5-2152-4d16-9589-bada75744837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%run ../Transversal/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67c54e6-d350-4384-857e-e7e1b89e3468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "neighborhoods_spark_df = spark.table(Bronze_Geodata)\n",
    "gdf_neighborhoods = spark_to_geopandas(df_spark=neighborhoods_spark_df)\n",
    "\n",
    "\n",
    "polygons = [\n",
    "    (row['IDENTIFICACION'], row['NOMBRE'], shape(row['geometry']))\n",
    "    for _, row in gdf_neighborhoods.iterrows()\n",
    "]\n",
    "\n",
    "broadcast_polygons = spark.sparkContext.broadcast(polygons)\n",
    "\n",
    "def get_comuna_barrio(lat, lon):\n",
    "\n",
    "    point = Point(float(lon), float(lat))\n",
    "\n",
    "    for comuna, barrio, poly in broadcast_polygons.value:\n",
    "        if poly.contains(point):\n",
    "            return (str(comuna), str(barrio))\n",
    "            \n",
    "    return ('DESCONOCIDA', 'DESCONOCIDO')\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"comuna\", StringType(), True),\n",
    "    StructField(\"barrio\", StringType(), True),\n",
    "])\n",
    "\n",
    "comuna_barrio_udf = udf(get_comuna_barrio, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ead6d6-eb07-445f-82fe-01ca9fe0792f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, date_format, year, month, dayofmonth, hour, minute, second\n",
    "import time\n",
    "\n",
    "\n",
    "def silver_stream(table_bronze, checkpoint, table_silver):\n",
    "\n",
    "    bronze_stream = (\n",
    "        spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .table(table_bronze)\n",
    "    )\n",
    "\n",
    "    clean_stream_df = bronze_stream.dropDuplicates([\"order_id\"])\n",
    "\n",
    "    df_with_timestamp = clean_stream_df.withColumn('event_timestamp', to_timestamp(col('date'), 'dd/MM/yyyy HH:mm:ss'))\n",
    "    df_with_geodata  = df_with_timestamp.withColumn(\"location_info\", comuna_barrio_udf(col(\"latitude\"), col(\"longitude\")))\\\n",
    "                                        .withColumn(\"comuna\", col(\"location_info.comuna\"))\\\n",
    "                                        .withColumn(\"barrio\", col(\"location_info.barrio\"))\\\n",
    "                                        .drop(\"location_info\")\n",
    "\n",
    "    silver_df = df_with_geodata.select(\n",
    "        date_format(col(\"event_timestamp\"), \"ddMMyyyy\").alias(\"partition_date\"),\n",
    "        col(\"order_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"employee_id\"),\n",
    "        col(\"quantity_products\"),\n",
    "        col(\"latitude\"),\n",
    "        col(\"longitude\"),\n",
    "        col(\"comuna\").cast(StringType()).alias(\"district\"),\n",
    "        col(\"barrio\").cast(StringType()).alias(\"neighborhood\"),\n",
    "        col(\"date\").alias(\"event_date\"),\n",
    "        year(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_year\"),\n",
    "        month(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_month\"),\n",
    "        dayofmonth(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_day\"),\n",
    "        hour(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_hour\"),\n",
    "        minute(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_minute\"),\n",
    "        second(col(\"event_timestamp\")).cast(IntegerType()).alias(\"event_second\")\n",
    "    )\n",
    "\n",
    "    query = (\n",
    "        silver_df.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint)\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .trigger(once=True)\n",
    "            .toTable(table_silver)\n",
    "    )\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "query = silver_stream(\n",
    "        table_bronze=Bronze_Orders,\n",
    "        checkpoint=checkpoint_silver,\n",
    "        table_silver=Silver_Orders\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver streaming",
   "widgets": {}
  },
  "dependencies": {
   "environment": {
    "environmentId": "913dd7af-cd09-411a-93a1-ccfc2c7af3d7",
    "workspaceId": "19f7c29b-83a9-42bb-812f-ef7bfc71c961"
   },
   "lakehouse": {
    "default_lakehouse": "5d727189-ca80-47e8-b94d-a7289df703a2",
    "default_lakehouse_name": "unalwater",
    "default_lakehouse_workspace_id": "19f7c29b-83a9-42bb-812f-ef7bfc71c961",
    "known_lakehouses": [
     {
      "id": "5d727189-ca80-47e8-b94d-a7289df703a2"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
